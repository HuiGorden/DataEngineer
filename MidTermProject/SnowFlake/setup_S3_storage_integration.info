Our personal S3 is set to private and choose to use Storage Integration to dump data to person S3.

AWS Side:
1. Use root user to login AWS console
2. We could create delicated S3 policy by providing json definition. For now just pick the existing S3FullAccess policy 
3. Create a role named "myrole" as trusted entity, and select "Required external ID" put 0000 for now. Record role ARN
4. Create bucket name "hui-mid-term"

Back to SnowFlake Side:
4. Create Storage Intergartion
CREATE STORAGE INTEGRATION s3_midterm
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = 'S3'
  ENABLED = TRUE
  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::056698095520:role/myrole'
  STORAGE_ALLOWED_LOCATIONS = ('s3://hui-mid-term/')

5. DESC STORAGE INTEGRATION s3_midterm to check created IAM user, this IAM users is created by SnowFlake

Back to AWS Side:
6. Edit Trust relation for myrole.{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "",
      "Effect": "Allow",
      "Principal": {
        "AWS": "<snowflake_user_arn>" ---> Need Update!
      },
      "Action": "sts:AssumeRole",
      "Condition": {
        "StringEquals": {
          "sts:ExternalId": "<snowflake_external_id>" ---> Need Update!
        }
      }
    }
  ]
}

Back to SnowFlake Side:
7. Grant privilege. "CREATE STAGE" privilege for the schema, "USAGE" privilege on storage integration
grant create stage on schema RAW to role ACCOUNTADMIN;
grant usage on integration s3_midterm to role ACCOUNTADMIN;

8. Create the stage
use schema MIDTERM_DB.RAW;

create stage my_s3_stage
  storage_integration = s3_midterm
  url = 's3://hui-mid-term/input/'

9. Unit Test dump data to S3
create or replace file format csv_comma_format
type = 'CSV'
field_delimiter = ',';

copy into '@MY_S3_STAGE/2022-12-13/inventory.csv' from (select * from midterm_db.raw.inventory where cal_dt <= current_date())
file_format=(TYPE=CSV, COMPRESSION='None')
single = true
MAX_FILE_SIZE=107772160
OVERWRITE=TRUE
HEADER=TRUE
;