1. Restart EC2 instances
2. # Remove all exited container
docker container rm connect-msk
docker container rm nifi
docker container rm mysql
3. # Setup MySQL
docker run -dit --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=debezium -e MYSQL_USER=mysqluser -e MYSQL_PASSWORD=mysqlpw debezium/example-mysql:1.6
docker exec -it mysql bash
mysql -u root -p # password for root is debezium

CREATE DATABASE demo;
use demo;

CREATE TABLE bus_status (
    record_id INT NOT NULL AUTO_INCREMENT,
    id INT NOT NULL,
    routeId INT NOT NULL,
    directionId VARCHAR(40),
    predictable BOOLEAN,
    secsSinceReport INT NOT NULL,
    kph INT NOT NULL,
    heading INT,
    lat REAL NOT NULL, 
    lon REAL NOT NULL,
    leadingVehicleId INT,
    event_time DATETIME DEFAULT NOW(),
    PRIMARY KEY (record_id)
);

describe bus_status;

4. # Setup Nifi 
docker run --name nifi -p 8080:8080 -p 8443:8443 --link mysql:mysql -d apache/nifi:1.12.0
Go to hostname:8080/nifi and configure processor
# Create mysql connector jar
docker exec -it nifi bash
mkdir custom-jars
cd custom-jars
wget http://java2s.com/Code/JarDownload/mysql/mysql-connector-java-5.1.17-bin.jar.zip
unzip mysql-connector-java-5.1.17-bin.jar.zip
upload Nifi template
reset JDBC connector password for user root
enable JDBC connector in nifi

5. # Create MSK
Custom Create
provisioned
version 2.6.2
create msk configuration, change "auto.create.topics.enable" to true
Unauthenticated access
plaintext between clients and brokers
uncheck "TLS encryption" within cluster
choose "DataEngineeringLearning" as security group
use aws managed key to encrypt data

# Use telnet to verfiy broker connectivity
telnet b-2.finalprojectmsk.y1yepf.c14.kafka.us-east-1.amazonaws.com 9092

# Use Kafka Cli Tool to test interaction with MSK
BOOTSTRAP_SERVERS=b-2.finalproject.34zas9.c3.kafka.ca-central-1.amazonaws.com:9092,b-3.finalproject.34zas9.c3.kafka.ca-central-1.amazonaws.com:9092,b-1.finalproject.34zas9.c3.kafka.ca-central-1.amazonaws.com:9092
cd ~/kafka_2.12-2.6.2/bin/
./kafka-topics.sh --list --bootstrap-server=$BOOTSTRAP_SERVERS

6. # Create Debezium CDC, to push data from MySQL to MSK 
docker run -dit --name connect-msk -p 8083:8083 -e GROUP_ID=1 -e CONFIG_STORAGE_TOPIC=my-connect-configs -e OFFSET_STORAGE_TOPIC=my-connect-offsets -e STATUS_STORAGE_TOPIC=my_connect_statuses -e BOOTSTRAP_SERVERS=$BOOTSTRAP_SERVERS -e KAFKA_VERSION=2.6.2 -e CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR=2 -e CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR=2 -e CONNECT_STATUS_STORAGE_REPLICATION_FACTOR=2 --link mysql:mysql debezium/connect:1.8.0.Final

./kafka-topics.sh --list --bootstrap-server=$BOOTSTRAP_SERVERS # check if CDC topic is created on MSK cluster

./kafka-console-consumer.sh  --bootstrap-server $BOOTSTRAP_SERVERS --topic my-connect-configs --from-beginning # check if CDC could write data to MSK cluster

curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" localhost:8083/connectors/ -d '{ "name": "bus-connector", "config": { "connector.class": "io.debezium.connector.mysql.MySqlConnector", "tasks.max": "1", "database.hostname": "mysql", "database.port": "3306", "database.user": "root", "database.password": "debezium", "database.server.id": "184054", "database.server.name": "dbserver1", "database.include.list": "demo", "database.history.kafka.bootstrap.servers": "'"$BOOTSTRAP_SERVERS"'", "database.history.kafka.topic": "dbhistory.demo" } }'

# check if CDC push bus data to MSK cluster
./kafka-console-consumer.sh  --bootstrap-server $BOOTSTRAP_SERVERS  --topic dbserver1.demo.bus_status 

7. # Create EMR Cluster
Amazon EMR version:emr-6.8.0(Hadoop3.2.1 + Hive 3.1.3 + Hue 4.10.0 + Spark 3.3.0 + Pig 0.17.0)
Use for Hive table metadata
Use for Spark table metadata
m4.2xlarge
Auto-termination 1 hour
uncheck Termination Protection
Choose EC2 key pair
EC2 instance profile: EC2DevelopmentRole
Spark Version:3.3.0(Scala:2.12.15)

8.
update MSK bootstrap server address in sparkScript.py
aws s3 cp sparkScript.py s3://hui-final-project/sparkScript.py
ssh to EMR master node as hadoop
spark-submit --master yarn --deploy-mode cluster --name hui-stremaing-app --jars /usr/lib/hudi/hudi-spark-bundle.jar --conf "spark.serializer=org.apache.spark.serializer.KryoSerializer" --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog"  --conf "spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension" --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 s3://hui-final-project/sparkScript.py

9.
cd ~
docker pull stantaov/superset-athena:0.0.1
docker run -d -p 8088:8088 --name superset stantaov/superset-athena:0.0.1
docker exec -it superset superset fab create-admin --username admin --firstname Superset --lastname Admin --email admin@superset.com --password admin
docker exec -it superset superset db upgrade
docker exec -it superset superset load_examples
docker exec -it superset superset init

# Create Connector to Athena
login superset dashboard <ec2_hostname>:8088
Data->Databases->Add Database icon
awsathena+rest://<Access key ID>:<Secret access key>@athena.us-east-1.amazonaws.com/final_project?s3_staging_dir=s3://finalproject-athena-query-result/&work_group=primary

Create Dataset -> bus_status
Create Chart busMap(Scatterplot)
Create Dashboard and put chart on it. 
Scatterplot, 1km Radius
Set dashboard auto-refresh to 10s